{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In \\[1\\]:\n",
    "\n",
    "    # here we are importing the reqiuired libraries \n",
    "\n",
    "    import warnings\n",
    "    warnings.filterwarnings('ignore')\n",
    "    import pandas as pd\n",
    "\n",
    "In \\[2\\]:\n",
    "\n",
    "    # Need to run this before importing XGboost\n",
    "    # here we have imported the sys library \n",
    "\n",
    "    import sys\n",
    "    !{sys.executable} -m pip install xgboost\n",
    "\n",
    "    Requirement already satisfied: xgboost in c:\\users\\riyanriya\\anaconda3\\lib\\site-packages (1.7.2)\n",
    "    Requirement already satisfied: scipy in c:\\users\\riyanriya\\anaconda3\\lib\\site-packages (from xgboost) (1.6.2)\n",
    "    Requirement already satisfied: numpy in c:\\users\\riyanriya\\anaconda3\\lib\\site-packages (from xgboost) (1.22.4)\n",
    "\n",
    "In \\[3\\]:\n",
    "\n",
    "    # Importing required packages\n",
    "\n",
    "    from numpy import loadtxt\n",
    "    # Python NumPy loadtxt() function is used to load the data from a\n",
    "    # text file and store them in a ndarray.\n",
    "\n",
    "    from xgboost import XGBClassifier\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from sklearn.metrics import accuracy_score\n",
    "\n",
    "In \\[4\\]:\n",
    "\n",
    "    # Importing various models to compare\n",
    "\n",
    "    from sklearn.linear_model import LogisticRegression\n",
    "    from sklearn.neighbors import KNeighborsClassifier\n",
    "    from sklearn.svm import SVC\n",
    "\n",
    "In \\[5\\]:\n",
    "\n",
    "    # Importing required packages\n",
    "\n",
    "    import warnings\n",
    "    warnings.filterwarnings('ignore')\n",
    "\n",
    "    # Bagged Decision Trees for Classification\n",
    "    # Importing required packages\n",
    "    import pandas as pd\n",
    "    from sklearn import model_selection\n",
    "    from sklearn.ensemble import BaggingClassifier\n",
    "    from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "    # here we are extracting the github user that had uploaded the dataset \n",
    "    # then we have given the names \n",
    "\n",
    "    url = \"https://raw.githubusercontent.com/jbrownlee/Datasets/master/pima-indians-diabetes.data.csv\"\n",
    "    names = ['preg', 'plas', 'pres', 'skin', 'test', 'mass', 'pedi', 'age', 'class']\n",
    "\n",
    "    # here we have created the variable called dataframe \n",
    "    # by using the pandas library we are reading the csv file  in which we have included \n",
    "    # the url and the names \n",
    "    # then we are checking for the dataframe \n",
    "\n",
    "    dataframe = pd.read_csv(url, names=names)\n",
    "    dataframe\n",
    "\n",
    "Out\\[5\\]:\n",
    "\n",
    "|     | preg | plas | pres | skin | test | mass | pedi  | age | class |\n",
    "|-----|------|------|------|------|------|------|-------|-----|-------|\n",
    "| 0   | 6    | 148  | 72   | 35   | 0    | 33.6 | 0.627 | 50  | 1     |\n",
    "| 1   | 1    | 85   | 66   | 29   | 0    | 26.6 | 0.351 | 31  | 0     |\n",
    "| 2   | 8    | 183  | 64   | 0    | 0    | 23.3 | 0.672 | 32  | 1     |\n",
    "| 3   | 1    | 89   | 66   | 23   | 94   | 28.1 | 0.167 | 21  | 0     |\n",
    "| 4   | 0    | 137  | 40   | 35   | 168  | 43.1 | 2.288 | 33  | 1     |\n",
    "| ... | ...  | ...  | ...  | ...  | ...  | ...  | ...   | ... | ...   |\n",
    "| 763 | 10   | 101  | 76   | 48   | 180  | 32.9 | 0.171 | 63  | 0     |\n",
    "| 764 | 2    | 122  | 70   | 27   | 0    | 36.8 | 0.340 | 27  | 0     |\n",
    "| 765 | 5    | 121  | 72   | 23   | 112  | 26.2 | 0.245 | 30  | 0     |\n",
    "| 766 | 1    | 126  | 60   | 0    | 0    | 30.1 | 0.349 | 47  | 1     |\n",
    "| 767 | 1    | 93   | 70   | 31   | 0    | 30.4 | 0.315 | 23  | 0     |\n",
    "\n",
    "768 rows × 9 columns\n",
    "\n",
    "In \\[6\\]:\n",
    "\n",
    "    # here we are checking the information present in the dataframe \n",
    "\n",
    "    dataframe.info()\n",
    "\n",
    "    <class 'pandas.core.frame.DataFrame'>\n",
    "    RangeIndex: 768 entries, 0 to 767\n",
    "    Data columns (total 9 columns):\n",
    "     #   Column  Non-Null Count  Dtype  \n",
    "    ---  ------  --------------  -----  \n",
    "     0   preg    768 non-null    int64  \n",
    "     1   plas    768 non-null    int64  \n",
    "     2   pres    768 non-null    int64  \n",
    "     3   skin    768 non-null    int64  \n",
    "     4   test    768 non-null    int64  \n",
    "     5   mass    768 non-null    float64\n",
    "     6   pedi    768 non-null    float64\n",
    "     7   age     768 non-null    int64  \n",
    "     8   class   768 non-null    int64  \n",
    "    dtypes: float64(2), int64(7)\n",
    "    memory usage: 54.1 KB\n",
    "\n",
    "In \\[ \\]:\n",
    "\n",
    "    #  observations - \n",
    "    #  non null tells that there is no missing values \n",
    "\n",
    "In \\[ \\]:\n",
    "\n",
    "    #  BAGGING CLASSIFIER - RANDOM FOREST \n",
    "\n",
    "In \\[7\\]:\n",
    "\n",
    "    # balanced data - this means equal proportion of the either class  that class 1na dclass 0 \n",
    "    # the  proportion 1 and the proportion of 0 should be equal in proportionality \n",
    "    # here we are counting the total value counts of the column class present in the dataframe \n",
    "\n",
    "    dataframe[\"class\"].value_counts()   \n",
    "\n",
    "    #  observations \n",
    "    #  as we can see there are total 500 zeros and the 268 ones \n",
    "    #  so this is the balanced data as we are not having the far difference \n",
    "\n",
    "Out\\[7\\]:\n",
    "\n",
    "    0    500\n",
    "    1    268\n",
    "    Name: class, dtype: int64\n",
    "\n",
    "In \\[8\\]:\n",
    "\n",
    "    # spliting data into independent and dependent features\n",
    "    #  here we are 2 variables X nad Y \n",
    "    #  here we are selecting all the rows and the column class in the y set \n",
    "    #  here we are selecting everything but drop the class variable as class is the dependent variable \n",
    "    # axis=1,means we are referring to columns(to drop)\n",
    "    # here we have axis = 1 that means we are drppoing the columns \n",
    "\n",
    "    y = dataframe.loc[:, \"class\"]\n",
    "    X = dataframe.drop(\"class\",axis=1)\n",
    "\n",
    "    # then we are importing the package \n",
    "    # then we are having the 4 subsets that is  X_train, X_test, y_train, y_test \n",
    "    # then we are applying the train test split function on the x and y \n",
    "    # using the test size of 0.2 that means 80/ 20 split \n",
    "    # and given the random stae of 42 to get the premanent output \n",
    "\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y,test_size =0.2,random_state=42)\n",
    "\n",
    "In \\[9\\]:\n",
    "\n",
    "    # here we have createdd an object / variable classifier from the class classifier \n",
    "    # () - it represents that we are choosing the default value of the hyperparametre  \n",
    "    # but the class have the hyperparameter \n",
    "    # then we are fitting the x train and the y train set on the classifier \n",
    "    # then we are predicting the y \n",
    "    # then we are predicting the x test  result through the classifier and storing it \n",
    "    # into the y predictions \n",
    "\n",
    "    classifier =XGBClassifier()\n",
    "    classifier.fit(X_train,y_train)\n",
    "    y_pred = classifier.predict(X_test)\n",
    "\n",
    "    # here we have imported the classification_report, confusion_matrix module from the sklearn.metrics \n",
    "    # then we are printing the confusion matrix  for the y test and the y predictions \n",
    "    # and then we are printing the classification report for the y test and the y prediction \n",
    "\n",
    "    from sklearn.metrics import classification_report, confusion_matrix\n",
    "    print(confusion_matrix(y_test, y_pred))\n",
    "    print(classification_report(y_test, y_pred))\n",
    "\n",
    "    [[70 29]\n",
    "     [19 36]]\n",
    "                  precision    recall  f1-score   support\n",
    "\n",
    "               0       0.79      0.71      0.74        99\n",
    "               1       0.55      0.65      0.60        55\n",
    "\n",
    "        accuracy                           0.69       154\n",
    "       macro avg       0.67      0.68      0.67       154\n",
    "    weighted avg       0.70      0.69      0.69       154\n",
    "\n",
    "In \\[10\\]:\n",
    "\n",
    "    # here we are importing the cross validation score module from the sklearn.model_selection package \n",
    "    # 1st do the cross validation and then measures the accuracy \n",
    "    # here we have created the variable called accuracies and we are storing the cross validation\n",
    "    # score in it \n",
    "    # estimator is the classifier \n",
    "    # then we have given the training set as x train and the testing set as the y test \n",
    "    # and the total number of folds will be 10  that means 10 samples we are creating with the entire \n",
    "    # dataset \n",
    "    # as we are having the 80 percent training data that we have broken up into the  10 samples \n",
    "    # each 10 part will be having the different combination of the 80 percent data . ideally we are \n",
    "    # creating the 10 training samples . \n",
    "    # each training sample will give the training accuracy score \n",
    "    # then we are printing the accuracy \n",
    "    # here we are formatting the output (\"Accuracy: {:.2f} %\".format(accuracies.mean()*100) -\n",
    "    # 1st mention the accuracy  then give the colon ,then within codes .2f - \n",
    "    # i should be float number but give me the results only to the two places of decimals \n",
    "    # and we are  formatting  on the mean accuracy \n",
    "    # because we are mentioning the text accuracy and then we have given the codes , \n",
    "    # quotations we will give when we are mentioning the text  , \n",
    "    # so the python understands that this is the text otherwise it will give the error \n",
    "\n",
    "    from sklearn.model_selection import cross_val_score\n",
    "    accuracies = cross_val_score(estimator = classifier,X = X_train, y = y_train, cv = 10)\n",
    "    print(\"Accuracy: {:.2f} %\".format(accuracies.mean()*100))\n",
    "    print(\"Standard Deviation: {:.2f} %\".format(accuracies.std()*100))\n",
    "\n",
    "    Accuracy: 75.56 %\n",
    "    Standard Deviation: 2.94 %\n",
    "\n",
    "In \\[11\\]:\n",
    "\n",
    "    # all 10 accuracies\n",
    "    # here we are checking the accurarcies for the 10 samples that we have created \n",
    "\n",
    "    accuracies\n",
    "\n",
    "Out\\[11\\]:\n",
    "\n",
    "    array([0.75806452, 0.77419355, 0.79032258, 0.79032258, 0.68852459,\n",
    "           0.75409836, 0.7704918 , 0.72131148, 0.75409836, 0.75409836])\n",
    "\n",
    "In \\[12\\]:\n",
    "\n",
    "    # For hyperparameter tuning we all the parameters\n",
    "    # here we are finding the hyperparametres \n",
    "    # params means pramatres for the particular object \n",
    "    #here we are checking to get all the parametres  for the object created that is classifier \n",
    "\n",
    "    classifier.get_params\n",
    "\n",
    "Out\\[12\\]:\n",
    "\n",
    "    <bound method XGBModel.get_params of XGBClassifier(base_score=0.5, booster='gbtree', callbacks=None,\n",
    "                  colsample_bylevel=1, colsample_bynode=1, colsample_bytree=1,\n",
    "                  early_stopping_rounds=None, enable_categorical=False,\n",
    "                  eval_metric=None, feature_types=None, gamma=0, gpu_id=-1,\n",
    "                  grow_policy='depthwise', importance_type=None,\n",
    "                  interaction_constraints='', learning_rate=0.300000012,\n",
    "                  max_bin=256, max_cat_threshold=64, max_cat_to_onehot=4,\n",
    "                  max_delta_step=0, max_depth=6, max_leaves=0, min_child_weight=1,\n",
    "                  missing=nan, monotone_constraints='()', n_estimators=100,\n",
    "                  n_jobs=0, num_parallel_tree=1, predictor='auto', random_state=0, ...)>\n",
    "\n",
    "In \\[13\\]:\n",
    "\n",
    "    # Grid search will run the model with all different combinations of parameters given below; \n",
    "    # and provide us the best result\n",
    "    # here we have created the variable called parametres \n",
    "    # then we have the range for the learning rate  and  also given the range for the \n",
    "    # another hyperparametrer that is gamma \n",
    "    # the value of gamma should as low as possible \n",
    "    # the lowest gamma value is 0 \n",
    "    # XG boost is also the collection of trees like as the random forest . \n",
    "    # the learning process is different \n",
    "    # when we are inclusing the new tree or splittng the decision tree there may be the problem of the \n",
    "    # overfitting \n",
    "    # ideally we want the loss to be minimum that is 0 \n",
    "    # 01 - it is relaxing the model to check the performance of the model \n",
    "    # 02 - it means giving some more relaxation \n",
    "    # loss - any kind of error in the model \n",
    "\n",
    "    parameters = [{'learning_rate': [0.3, 0.5], 'gamma': [0,.01,.02]}]\n",
    "\n",
    "In \\[14\\]:\n",
    "\n",
    "    # here we are importing the required libraries \n",
    "\n",
    "    from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "    grid_search = GridSearchCV(estimator = classifier,param_grid = parameters,scoring = \n",
    "                               'accuracy',cv = 10, n_jobs = -1)\n",
    "\n",
    "    # n_jobs = -1 means the Gridsearch will utilize all the available cores of your computer\n",
    "\n",
    "In \\[15\\]:\n",
    "\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    best_accuracy = grid_search.best_score_\n",
    "    best_parameters = grid_search.best_params_\n",
    "    print(\"Best Accuracy: {:.2f} %\".format(best_accuracy*100))\n",
    "    print(\"Best Parameters:\", best_parameters)\n",
    "\n",
    "    Best Accuracy: 76.21 %\n",
    "    Best Parameters: {'gamma': 0.02, 'learning_rate': 0.5}\n",
    "\n",
    "## Regularization is any modiﬁcation we make to a learning algorithm that is intended to reduce its generalization error but not its training error.<a href=\"#Regularization-is-any-modi%EF%AC%81cation-we-make-to-a-learning-algorithm-that-is-intended-to-reduce-its-generalization-error-but-not-its-training-error.\" class=\"anchor-link\">¶</a>\n",
    "\n",
    "## When training a learning algorithm, while we care about its performance on the dataset used to train it, we mostly care about its ability to generalize, i.e., the quality of its predictions on data not used in the training process. Without taking proper measures, a learning algorithm can overfit to its training data, meaning that while it performs well on this dataset, its performance is significantly worse on out-of-sample observations. Regularization is any means by which we improve a learning algorithm’s ability to generalize, possibly (and usually) at the expense of its performance on the training data.<a href=\"#When-training-a-learning-algorithm,-while-we-care-about-its-performance-on-the-dataset-used-to-train-it,-we-mostly-care-about-its-ability-to-generalize,-i.e.,-the-quality-of-its-predictions-on-data-not-used-in-the-training-process.-Without-taking-proper-measures,-a-learning-algorithm-can-overfit-to-its-training-data,-meaning-that-while-it-performs-well-on-this-dataset,-its-performance-is-significantly-worse-on-out-of-sample-observations.-Regularization-is-any-means-by-which-we-improve-a-learning-algorithm%E2%80%99s-ability-to-generalize,-possibly-(and-usually)-at-the-expense-of-its-performance-on-the-training-data.\" class=\"anchor-link\">¶</a>\n",
    "\n",
    "## Gradient boosting involves creating and adding trees to the model sequentially.<a href=\"#Gradient-boosting-involves-creating-and-adding-trees-to-the-model-sequentially.\" class=\"anchor-link\">¶</a>\n",
    "\n",
    "## New trees are created to correct the residual errors in the predictions from the existing sequence of trees.<a href=\"#New-trees-are-created-to-correct-the-residual-errors-in-the-predictions-from-the-existing-sequence-of-trees.\" class=\"anchor-link\">¶</a>\n",
    "\n",
    "## The effect is that the model can quickly fit, then overfit the training dataset.<a href=\"#The-effect-is-that-the-model-can-quickly-fit,-then-overfit-the-training-dataset.\" class=\"anchor-link\">¶</a>\n",
    "\n",
    "## A technique to slow down the learning in the gradient boosting model is to apply a weighting factor for the corrections by new trees when added to the model.<a href=\"#A-technique-to-slow-down-the-learning-in-the-gradient-boosting-model-is-to-apply-a-weighting-factor-for-the-corrections-by-new-trees-when-added-to-the-model.\" class=\"anchor-link\">¶</a>\n",
    "\n",
    "## This weighting is called the shrinkage factor or the learning rate, depending on the literature or the tool.<a href=\"#This-weighting-is-called-the-shrinkage-factor-or-the-learning-rate,-depending-on-the-literature-or-the-tool.\" class=\"anchor-link\">¶</a>\n",
    "\n",
    "## A problem with gradient boosted decision trees is that they are quick to learn and overfit training data. One effective way to slow down learning in the gradient boosting model is to use a learning rate, also called shrinkage (or eta in XGBoost).<a href=\"#A-problem-with-gradient-boosted-decision-trees-is-that-they-are-quick-to-learn-and-overfit-training-data.-One-effective-way-to-slow-down-learning-in-the-gradient-boosting-model-is-to-use-a-learning-rate,-also-called-shrinkage-(or-eta-in-XGBoost).\" class=\"anchor-link\">¶</a>\n",
    "\n",
    "## The main goal should be to reduce the complexity as much as possible while retaining most of the performance during validation. That means the least number of trees and smallest depth you can get away without making the model worse. Over time, you’ll see that a simpler model will be more robust against overfitting. The more you reduce the number of trees, the more you should increase the learning rate, to make sure the algorithm converges.<a href=\"#The-main-goal-should-be-to-reduce-the-complexity-as-much-as-possible-while-retaining-most-of-the-performance-during-validation.-That-means-the-least-number-of-trees-and-smallest-depth-you-can-get-away-without-making-the-model-worse.-Over-time,-you%E2%80%99ll-see-that-a-simpler-model-will-be-more-robust-against-overfitting.-The-more-you-reduce-the-number-of-trees,-the-more-you-should-increase-the-learning-rate,-to-make-sure-the-algorithm-converges.\" class=\"anchor-link\">¶</a>\n",
    "\n",
    "In \\[16\\]:\n",
    "\n",
    "    #Running various models\n",
    "    models = []\n",
    "    models.append(('LogisticRegression', LogisticRegression()))\n",
    "    models.append(('KNN', KNeighborsClassifier()))\n",
    "    models.append(('SVM', SVC()))\n",
    "    models.append(('XGB',XGBClassifier(eta=0.01,gamma=10))) #eta = 0.01,gamma = 10\n",
    "\n",
    "    # it is a regularization parameter that shrinks feature weights in each boosting step. \n",
    "    #The default value is 0.3 but people generally tune with values such as 0.01, 0.1, 0.2\n",
    "\n",
    "    #The higher Gamma is, the higher the regularization.It is a hyperparameter which controls the \n",
    "    #complexity of the model\n",
    "\n",
    "\n",
    "\n",
    "    import time   #importing time models to capture the execution time of the models\n",
    "\n",
    "    # evaluate each model in turn\n",
    "    results = []\n",
    "    names = []\n",
    "    scoring = 'accuracy'\n",
    "\n",
    "    for name, model in models:\n",
    "        start_time = time.time()\n",
    "        model.fit(X_train, y_train)\n",
    "\n",
    "        y_pred = model.predict(X_test)\n",
    "        predictions = [round(value) for value in y_pred]\n",
    "\n",
    "\n",
    "        # evaluate predictions\n",
    "        accuracy = accuracy_score(y_test, predictions)\n",
    "        print(\"Accuracy: %.2f%%\" % (accuracy * 100.0),name)\n",
    "        print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "        \n",
    "\n",
    "    Accuracy: 75.97% LogisticRegression\n",
    "    --- 0.11117291450500488 seconds ---\n",
    "    Accuracy: 66.23% KNN\n",
    "    --- 0.015612363815307617 seconds ---\n",
    "    Accuracy: 76.62% SVM\n",
    "    --- 0.016031742095947266 seconds ---\n",
    "    Accuracy: 78.57% XGB\n",
    "    --- 0.13203167915344238 seconds ---\n",
    "\n",
    "## A machine learning model is the definition of a mathematical formula with a number of parameters that need to be learned from the data. That is the crux of machine learning: fitting a model to the data. This is done through a process known as model training. In other words, by training a model with existing data, we are able to fit the model parameters.<a href=\"#A-machine-learning-model-is-the-definition-of-a-mathematical-formula-with-a-number-of-parameters-that-need-to-be-learned-from-the-data.-That-is-the-crux-of-machine-learning:-fitting-a-model-to-the-data.-This-is-done-through-a-process-known-as-model-training.-In-other-words,-by-training-a-model-with-existing-data,-we-are-able-to-fit-the-model-parameters.\" class=\"anchor-link\">¶</a>\n",
    "\n",
    "## However, there is another kind of parameters that cannot be directly learned from the regular training process. These parameters express “higher-level” properties of the model such as its complexity or how fast it should learn. They are called hyperparameters. Hyperparameters are usually fixed before the actual training process begins.<a href=\"#However,-there-is-another-kind-of-parameters-that-cannot-be-directly-learned-from-the-regular-training-process.-These-parameters-express-%E2%80%9Chigher-level%E2%80%9D-properties-of-the-model-such-as-its-complexity-or-how-fast-it-should-learn.-They-are-called-hyperparameters.-Hyperparameters-are-usually-fixed-before-the-actual-training-process-begins.\" class=\"anchor-link\">¶</a>\n",
    "\n",
    "In \\[ \\]:\n",
    "\n",
    "    ## A machine learning model is the definition of a mathematical formula with \n",
    "    # a number of parameters that need to be learned from the data. \n",
    "    # That is the crux of machine learning: fitting a model to the data.\n",
    "    # This is done through a process known as model training. \n",
    "    # In other words, by training a model with existing data, we are able to fit the model parameters.\n",
    "\n",
    "    ## However, there is another kind of parameters that cannot be directly learned \n",
    "    # from the regular training process. These parameters express “higher-level” properties of the model\n",
    "    # such as its complexity or how fast it should learn. They are called hyperparameters. \n",
    "    # Hyperparameters are usually fixed before the actual training process begins.\n",
    "\n",
    "In \\[ \\]:\n",
    "\n",
    "     \n",
    "\n",
    "In \\[ \\]:"
   ]
  }
 ],
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {}
}
